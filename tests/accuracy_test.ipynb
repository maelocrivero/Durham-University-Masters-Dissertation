{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a976d814-71cb-45df-a793-7818c5b6fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import probabilitylib as pl\n",
    "from probabilitylib import ProbabilitySpace\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d35f5-9c36-458a-b0c7-13b68fec3f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to generate a synthetic dataset\n",
    "def generate_customer_data(num_records):\n",
    "    data = []\n",
    "\n",
    "    def maybe_nan(value):\n",
    "        return value if random.random() > 0.2 else np.nan  \n",
    "\n",
    "    for _ in range(num_records):\n",
    "        age = np.random.randint(18, 80)\n",
    "       \n",
    "        # Rule: Income is loosely based on age\n",
    "        base_income = 20000 + (age - 18) * 1000\n",
    "        income = np.random.normal(base_income, base_income * 0.2)\n",
    "       \n",
    "        # Rule: Credit score is influenced by age and income\n",
    "        credit_score = min(850, max(300, int(600 + (age/80)*100 + (income/100000)*100 + np.random.normal(0, 50))))\n",
    "       \n",
    "        # Rule: Loan amount is based on income and credit score\n",
    "        max_loan = income * (credit_score / 600)\n",
    "        loan_amount = np.random.uniform(0, max_loan)\n",
    "\n",
    "        gender = np.random.choice(['Male', 'Female'])\n",
    "\n",
    "        if age <= 30 and gender == \"Female\":\n",
    "            smoker_prob = 0.1\n",
    "        elif age <= 30 and gender == \"Male\":\n",
    "            smoker_prob = 0.05\n",
    "        elif age in range(31, 50) and gender == \"Female\":\n",
    "            smoker_prob = 0.35\n",
    "        elif age in range(31, 50) and gender == \"Male\":\n",
    "            smoker_prob = 0.25\n",
    "        elif age >= 51 and gender == \"Female\":\n",
    "            smoker_prob = 0.6\n",
    "        else:\n",
    "            smoker_prob = 0.5\n",
    "            \n",
    "        smoker = random.choices([True, False], weights=[smoker_prob, 1 - smoker_prob])[0]\n",
    "\n",
    "        weights = np.random.random()\n",
    "\n",
    "    \n",
    "        row = [\n",
    "            maybe_nan(age),\n",
    "            maybe_nan(income),\n",
    "            maybe_nan(credit_score),\n",
    "            maybe_nan(loan_amount),\n",
    "            maybe_nan(gender),\n",
    "            maybe_nan(smoker),\n",
    "            maybe_nan(weights)\n",
    "        ]\n",
    "        data.append(row)\n",
    "\n",
    "    return pd.DataFrame(data, columns=['Age', 'Income', 'CreditScore', 'LoanAmount', 'Gender', 'Smoker', 'weights'])\n",
    "\n",
    "#create a synthetic dataset with 500000 rows. \n",
    "df = generate_customer_data(500000)\n",
    "\n",
    "#print the first 5 rows of the dataset and its length\n",
    "print(df.head())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16d223-8d90-4166-b896-aeac3caa4fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#create a binary version of gender\n",
    "df[\"GenderNumeric\"] = df[\"Gender\"].map({\"Male\": 0, \"Female\": 1})\n",
    "\n",
    "#convert df to a probability space\n",
    "ps = ProbabilitySpace(df)\n",
    "\n",
    "#convert df to a csv file\n",
    "d = df.to_csv(\"example.csv\", index=False)\n",
    "print(\"CSV file 'example.csv' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa02cf2-86b3-48e8-8591-293738c323f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#find the average percentage error between expected mean and combine_expect_mean\n",
    "\n",
    "errors_expected_mean = []\n",
    "chunk_sizes_expected_mean = []\n",
    "variables = []\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    var = random.choice([\"Age\", \"Income\", \"CreditScore\", \"LoanAmount\", \"GenderNumeric\"])\n",
    "    variables.append(var)\n",
    "\n",
    "    res = pl.expected_mean(ps, var)\n",
    "    \n",
    "    # Random chunk size\n",
    "    m = np.random.randint(1000, 250000)\n",
    "    chunk_sizes_expected_mean.append((m/500000)*100)\n",
    "    \n",
    "    \n",
    "    chunk_results = pl.process_in_chunks(\n",
    "        \"example.csv\", m, \n",
    "        pl.expected_mean, \n",
    "        var\n",
    "    )\n",
    "    \n",
    "    # Combine counts to get global MI\n",
    "    combined_expected_mean = pl.combine_means(chunk_results)\n",
    "    \n",
    "    # Compute % error\n",
    "    error = abs(res - combined_expected_mean) / ((res + combined_expected_mean) / 2) * 100\n",
    "    errors_expected_mean.append(error)\n",
    "    \n",
    "    print(f\"Iteration {i} completed, chunk size={m}, error={error:.5f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f967e1-0b50-46e5-a70a-85b730d8b990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Average % error: 0.000148797903251%\n",
      "📉 Min error: 0.000000679097799%, Max error: 0.001095873825836%\n"
     ]
    }
   ],
   "source": [
    "#print the average error and the minimum and amximum errors\n",
    "print(f\"📊 Average % error: {np.mean(errors_expected_mean):.15f}%\")\n",
    "print(f\"📉 Min error: {np.min(errors_expected_mean):.15f}%, Max error: {np.max(errors_expected_mean):.15f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7869a6fe-3d18-4e8e-8948-9f5498d4f21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               count      mean       std           min       25%       50%  \\\n",
      "variable                                                                     \n",
      "Age             23.0  0.000121  0.000114  2.279881e-06  0.000052  0.000110   \n",
      "CreditScore     19.0  0.000013  0.000020  6.790978e-07  0.000002  0.000008   \n",
      "GenderNumeric   17.0  0.000230  0.000275  2.178752e-05  0.000065  0.000138   \n",
      "Income          20.0  0.000164  0.000168  2.724411e-05  0.000065  0.000089   \n",
      "LoanAmount      21.0  0.000221  0.000188  2.768506e-06  0.000124  0.000185   \n",
      "\n",
      "                    75%       max  \n",
      "variable                           \n",
      "Age            0.000139  0.000442  \n",
      "CreditScore    0.000014  0.000083  \n",
      "GenderNumeric  0.000207  0.001096  \n",
      "Income         0.000171  0.000644  \n",
      "LoanAmount     0.000261  0.000764  \n"
     ]
    }
   ],
   "source": [
    "#create DataFrame with columns containing the variables, chunk sizes selected, and the percentage error\n",
    "df_results = pd.DataFrame({\n",
    "    \"variable\": variables,\n",
    "    \"chunk_size\": chunk_sizes_expected_mean,\n",
    "    \"error_percent\": errors_expected_mean\n",
    "})\n",
    "\n",
    "#return a statistical breakdown of the DataFrame\n",
    "summary = df_results.groupby(\"variable\")[\"error_percent\"].describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54c7c2-dd08-45f2-96f7-a895675f087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute average percentage error per variable\n",
    "avg_errors = df_results.groupby(\"variable\")[\"error_percent\"].mean()\n",
    "\n",
    "#bar chart of errors per variable\n",
    "avg_errors.plot(kind=\"bar\", figsize=(8,5))\n",
    "plt.ylabel(\"Average % Error\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Average Percentage Error by Variable\")\n",
    "plt.savefig(\"expected_mean_deviation_by_variable.png\", dpi=300, bbox_inches='tight')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a890dc-f2d9-40c7-b6b9-04aedd9fa03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the effect of sample proportion on percentage error\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(chunk_sizes_expected_mean, errors_expected_mean, alpha=0.6)\n",
    "plt.xlabel(\"Percentage of rows sampled\")\n",
    "plt.ylabel(\"Percentage error (%)\")\n",
    "plt.title(\"Effect of sampling proportion on percentage error for expected_mean\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"chunk_error_plot_expected_mean.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a0d430-32fd-4916-989c-d6cc62571e4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run the same process for variance\n",
    "errors_variance = []\n",
    "chunk_sizes_variance = []\n",
    "variables = []\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    var = random.choice([\"Age\", \"Income\", \"CreditScore\", \"LoanAmount\", \"GenderNumeric\"])\n",
    "    variables.append(var)\n",
    "\n",
    "    res = pl.variance(ps, var)\n",
    "    \n",
    "    # Random chunk size\n",
    "    m = np.random.randint(1000, 250000)\n",
    "    chunk_sizes_variance.append((m/500000)*100)\n",
    "    \n",
    "    \n",
    "    chunk_results = pl.process_in_chunks(\n",
    "        \"example.csv\", m, \n",
    "        pl.variance, \n",
    "        var\n",
    "    )\n",
    "    \n",
    "    combined_variance = pl.combine_variance(chunk_results)\n",
    "    \n",
    "    error = abs(res - combined_variance) / ((res + combined_variance) / 2) * 100\n",
    "    errors_variance.append(error)\n",
    "    \n",
    "    print(f\"Iteration {i} completed, chunk size={m}, error={error:.5f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cde905fd-0ca1-4748-8ca0-a2ea88e6d505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Average % error: 0.000325093476322%\n",
      "📉 Min error: 0.000000104985450%, Max error: 0.003269092922550%\n"
     ]
    }
   ],
   "source": [
    "print(f\"📊 Average % error: {np.mean(errors_variance):.15f}%\")\n",
    "print(f\"📉 Min error: {np.min(errors_variance):.15f}%, Max error: {np.max(errors_variance):.15f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "231a8a6d-2f74-4200-8e87-ae62e84b9ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               count          mean           std           min           25%  \\\n",
      "variable                                                                       \n",
      "Age             22.0  2.071456e-04  2.188709e-04  6.077574e-06  6.724582e-05   \n",
      "CreditScore     20.0  2.713676e-04  5.211085e-04  1.366819e-05  6.871100e-05   \n",
      "GenderNumeric   15.0  8.535950e-07  6.692489e-07  1.049854e-07  4.371725e-07   \n",
      "Income          25.0  4.803559e-04  7.369742e-04  7.403765e-06  1.227769e-04   \n",
      "LoanAmount      18.0  5.835052e-04  7.919809e-04  5.445995e-05  1.282712e-04   \n",
      "\n",
      "                        50%       75%       max  \n",
      "variable                                         \n",
      "Age            1.206222e-04  0.000312  0.000922  \n",
      "CreditScore    1.042582e-04  0.000232  0.002401  \n",
      "GenderNumeric  5.627043e-07  0.000001  0.000002  \n",
      "Income         2.269174e-04  0.000367  0.003269  \n",
      "LoanAmount     2.933229e-04  0.000534  0.002908  \n"
     ]
    }
   ],
   "source": [
    "df_results_variance = pd.DataFrame({\n",
    "    \"variable\": variables,\n",
    "    \"chunk_size\": chunk_sizes_variance,\n",
    "    \"error_percent\": errors_variance\n",
    "})\n",
    "\n",
    "# 2. Summary statistics by variable\n",
    "summary = df_results_variance.groupby(\"variable\")[\"error_percent\"].describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef3664-e355-4a49-b61e-4e722d33ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_errors_variance = df_results_variance.groupby(\"variable\")[\"error_percent\"].mean()\n",
    "\n",
    "\n",
    "avg_errors_variance.plot(kind=\"bar\", figsize=(8,5))\n",
    "plt.ylabel(\"Average % Error\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Average Percentage Error by Variable\")\n",
    "plt.savefig(\"variance_deviation_by_variable.png\", dpi=300, bbox_inches='tight')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6819f54d-d681-48d6-bb25-d7551229f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(chunk_sizes_variance, errors_variance, alpha=0.6)\n",
    "plt.xlabel(\"Percentage of rows sampled\")\n",
    "plt.ylabel(\"Percentage error (%)\")\n",
    "plt.title(\"Effect of sampling proportion on percentage error for variance\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"chunk_error_plot_variance.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc083c-7104-44f9-ba21-ffc0f2db750a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#same process for IQR\n",
    "\n",
    "errors_IQR = []\n",
    "chunk_sizes_IQR = []\n",
    "sample_props_IQR = []\n",
    "variables = []\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    var = random.choice([\"Age\", \"Income\", \"CreditScore\", \"LoanAmount\"])\n",
    "    variables.append(var)\n",
    "\n",
    "    res = pl.IQR(ps, var)[\"IQR\"]\n",
    "    \n",
    "    # Random chunk size\n",
    "    m = np.random.randint(1000, 250000)\n",
    "    x = np.random.random() * 0.7\n",
    "    \n",
    "    chunk_sizes_IQR.append((m/500000)*100)\n",
    "\n",
    "    \n",
    "    chunk_results = pl.process_in_chunks(\n",
    "        \"example.csv\", m, \n",
    "        pl.IQR, \n",
    "        var,\n",
    "        sample_size=int(x*m)    \n",
    "    )\n",
    "\n",
    "    sample_props_IQR.append((int(x*m)/m)*100) \n",
    "    \n",
    "    combined_IQR = pl.combine_IQR(chunk_results, var)[\"IQR\"]\n",
    "    \n",
    "    error = abs(res - combined_IQR) / ((res + combined_IQR) / 2) * 100\n",
    "    errors_IQR.append(error)\n",
    "    \n",
    "    print(f\"Iteration {i} completed, error={error:.5f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba7869da-9ac2-4184-ad5b-71687315a1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Average % error: 0.247525532647710%\n",
      "📉 Min error: 0.000000000000000%, Max error: 2.319782785038674%\n"
     ]
    }
   ],
   "source": [
    "print(f\"📊 Average % error: {np.mean(errors_IQR):.15f}%\")\n",
    "print(f\"📉 Min error: {np.min(errors_IQR):.15f}%, Max error: {np.max(errors_IQR):.15f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c33549e8-2989-4fd3-8fd9-082f8f37487b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             count      mean       std           min       25%       50%  \\\n",
      "variable                                                                   \n",
      "Age           19.0  0.000000  0.000000  0.000000e+00  0.000000  0.000000   \n",
      "CreditScore   32.0  0.363419  0.520197  0.000000e+00  0.000000  0.000000   \n",
      "Income        30.0  0.324054  0.599675  2.571831e-11  0.045336  0.135034   \n",
      "LoanAmount    19.0  0.179028  0.231963  1.239035e-02  0.058896  0.090342   \n",
      "\n",
      "                  75%       max  \n",
      "variable                         \n",
      "Age          0.000000  0.000000  \n",
      "CreditScore  1.104972  1.117318  \n",
      "Income       0.201260  2.319783  \n",
      "LoanAmount   0.207094  1.052932  \n"
     ]
    }
   ],
   "source": [
    "df_results_IQR = pd.DataFrame({\n",
    "    \"variable\": variables,\n",
    "    \"chunk_size\": chunk_sizes_IQR,\n",
    "    \"error_percent\": errors_IQR,\n",
    "    \"sample_size\": sample_props_IQR\n",
    "})\n",
    "\n",
    "summary = df_results_IQR.groupby(\"variable\")[\"error_percent\"].describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca3dc3-edcf-432d-9eba-11d20f33c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_errors_IQR = df_results_IQR.groupby(\"variable\")[\"error_percent\"].mean()\n",
    "\n",
    "avg_errors_IQR.plot(kind=\"bar\", figsize=(8,5))\n",
    "plt.ylabel(\"Average % Error\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Average Percentage Error by Variable\")\n",
    "plt.savefig(\"IQR_deviation_by_variable.png\", dpi=300, bbox_inches='tight')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301105a-01e5-4b83-a6a3-438b72a664db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(chunk_sizes_IQR, errors_IQR, alpha=0.6)\n",
    "plt.xlabel(\"chunk_size\")\n",
    "plt.ylabel(\"Percentage error (%)\")\n",
    "plt.title(\"Effect of chunking proportion on percentage error for IQR\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"chunk_error_plot_IQR.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012cfe0c-6193-4a7f-9ae7-b17660606a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(sample_props_IQR, errors_IQR, alpha=0.6)\n",
    "plt.xlabel(\"Percentage of rows sampled\")\n",
    "plt.ylabel(\"Percentage error (%)\")\n",
    "plt.title(\"Effect of sampling proportion on percentage error for IQR\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"sampling_error_plot_IQR.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12dc0f5-df9f-40f5-9e91-d14bd28ddf6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#same process for mutual information\n",
    "\n",
    "#start with analysing mutual information with pairs of discrete variables\n",
    "\n",
    "errors_mutual_information_discrete = []\n",
    "chunk_sizes_mutual_information_discrete = []\n",
    "variables = [\"Gender\", \"GenderNumeric\", \"Smoker\"]\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "for i in range(100):\n",
    "    iter_start = time.time()\n",
    "    \n",
    "    x = random.choice(variables)\n",
    "    y = random.choice([v for v in variables if v != x])\n",
    "\n",
    "    res = pl.mutual_information(ps, x, y)\n",
    "    \n",
    "    # Random chunk size\n",
    "    m = np.random.randint(1000, 250000)\n",
    "    chunk_sizes_mutual_information_discrete.append((m/500000)*100)\n",
    "    \n",
    "    chunk_results = pl.process_in_chunks(\n",
    "        \"example.csv\", m, \n",
    "        pl.mutual_information, \n",
    "        x,\n",
    "        y   \n",
    "    )\n",
    "    combined_MI = pl.combine_mutual_information(chunk_results)\n",
    "    \n",
    "    error = abs(res - combined_MI) / ((res + combined_MI) / 2) * 100\n",
    "    errors_mutual_information_discrete.append(error)\n",
    "    \n",
    "    print(f\"Iteration {i} completed, chunk size={m}, error={error:.5f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf35cbdf-e049-4e30-8329-728a93be07d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Average % error: 0.000000000001760%\n",
      "📉 Min error: 0.000000000000000%, Max error: 0.000000000013367%\n"
     ]
    }
   ],
   "source": [
    "print(f\"📊 Average % error: {np.mean(errors_mutual_information_discrete):.15f}%\")\n",
    "print(f\"📉 Min error: {np.min(errors_mutual_information_discrete):.15f}%, Max error: {np.max(errors_mutual_information_discrete):.15f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b1f202-34e0-4b17-bd0f-9bcf59ae08da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#analyse percentage error when both variables are continuous\n",
    "\n",
    "errors_mutual_information_continuous = []\n",
    "chunk_sizes_mutual_information_continuous = []\n",
    "variables = [\"Age\", \"Income\", \"CreditScore\", \"LoanAmount\"]\n",
    "var_list = []\n",
    "\n",
    "for i in range(100):\n",
    "   \n",
    "    m = np.random.randint(1000, 250000)\n",
    "    chunk_sizes_mutual_information_continuous.append((m/500000)*100)\n",
    "\n",
    "    x = random.choice(variables)\n",
    "    y = random.choice([v for v in variables if v != x])\n",
    "\n",
    "    var_list.append([x, y])\n",
    "    \n",
    "    #regular function use to avoid loop having to use binning_in_chunks repeatedly\n",
    "    bin_boundaries = pl.bin_continuous_columns(ps, [x, y], return_edges=True)\n",
    "\n",
    "    res = pl.mutual_information(ps, x, y, bin_boundaries=bin_boundaries)\n",
    "    \n",
    "    chunk_results = pl.process_in_chunks(\n",
    "        \"example.csv\", m, \n",
    "        pl.mutual_information, \n",
    "        x,\n",
    "        y,\n",
    "        bin_boundaries=bin_boundaries\n",
    "    )\n",
    "    \n",
    "    combined_MI = pl.combine_mutual_information(chunk_results)\n",
    "    \n",
    "    error = abs(res - combined_MI) / ((res + combined_MI) / 2) * 100\n",
    "    errors_mutual_information_continuous.append(error)\n",
    "       \n",
    "    print(f\"Iteration {i} completed, chunk size={m}, error={error:.5f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f76bd16-0d9a-44b3-935c-c59641648db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Average % error: 0.000000000000137%\n",
      "📉 Min error: 0.000000000000000%, Max error: 0.000000000000598%\n"
     ]
    }
   ],
   "source": [
    "print(f\"📊 Average % error: {np.mean(errors_mutual_information_continuous):.15f}%\")\n",
    "print(f\"📉 Min error: {np.min(errors_mutual_information_continuous):.15f}%, Max error: {np.max(errors_mutual_information_continuous):.15f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685a1cd-115b-40b3-be38-777cb2726947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#finally find average error when one variable is continuous and one is discrete\n",
    "\n",
    "errors_mutual_information_mixed = []\n",
    "chunk_sizes_mutual_information_mixed = []\n",
    "x_vars = []\n",
    "y_vars = []\n",
    "\n",
    "con_variables = [\"Age\", \"Income\", \"CreditScore\", \"LoanAmount\"]\n",
    "discrete_variables = [\"Gender\", \"GenderNumeric\", \"Smoker\"]\n",
    "\n",
    "for i in range(100):\n",
    "    iter_start = time.time()\n",
    "    \n",
    "    m = np.random.randint(1000, 250000)\n",
    "    chunk_sizes_mutual_information_mixed.append((m/500000)*100)\n",
    "\n",
    "    x = random.choice(con_variables)\n",
    "    y = random.choice(discrete_variables)\n",
    "    x_vars.append(x)\n",
    "    y_vars.append(y)\n",
    "\n",
    "    bin_boundaries = pl.bin_continuous_columns(ps, [x], return_edges=True)\n",
    "\n",
    "    res = pl.mutual_information(ps, x, y, bin_boundaries=bin_boundaries)\n",
    "\n",
    "    chunk_results = pl.process_in_chunks(\n",
    "        \"example.csv\",\n",
    "        m, \n",
    "        pl.mutual_information, \n",
    "        x,\n",
    "        y,\n",
    "        bin_boundaries=bin_boundaries\n",
    "    )\n",
    "    \n",
    "    combined_MI = pl.combine_mutual_information(chunk_results)\n",
    "    \n",
    "    error = abs(res - combined_MI) / ((res + combined_MI) / 2) * 100\n",
    "    errors_mutual_information_mixed.append(error)\n",
    "    \n",
    "    print(f\"Iteration {i} completed, chunk size={m}, error={error:.5f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "194a8d4c-07f5-4442-a97e-200554b64b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Average % error: 0.000000004574002%\n",
      "📉 Min error: 0.000000000000019%, Max error: 0.000000043958147%\n"
     ]
    }
   ],
   "source": [
    "print(f\"📊 Average % error: {np.mean(errors_mutual_information_mixed):.15f}%\")\n",
    "print(f\"📉 Min error: {np.min(errors_mutual_information_mixed):.15f}%, Max error: {np.max(errors_mutual_information_mixed):.15f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd918a-54cf-41f2-a2f8-2f546ac06c6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#same process for conditional probability. Loop contains a range of variable combinations\n",
    "#ensuring both continuous and discrete variables are featured.\n",
    "\n",
    "errors_conditional = []\n",
    "chunk_sizes_conditional = []\n",
    "n = []\n",
    "\n",
    "for i in range(100):\n",
    "    x = random.choice([1, 2, 3, 4, 5])\n",
    "    n.append(x)\n",
    "    \n",
    "    m = np.random.randint(1000, 250000)\n",
    "    chunk_sizes_conditional.append((m/500000)*100)\n",
    "\n",
    "    if x==1:\n",
    "\n",
    "        res = pl.conditional(\n",
    "            ps,\n",
    "            event_condition = ps.df[\"Smoker\"] == True,\n",
    "            given_condition = ps.df[\"GenderNumeric\"] == 0\n",
    "        )\n",
    "               \n",
    "        chunk_results = pl.process_in_chunks(\n",
    "                \"example.csv\",\n",
    "                m,\n",
    "                pl.conditional,\n",
    "                lambda df: df[\"Smoker\"] == True,\n",
    "                lambda df: df[\"GenderNumeric\"] == 0\n",
    "            )\n",
    "        \n",
    "        combined_conditional = pl.combine_conditional(chunk_results)\n",
    "\n",
    "    if x==2:\n",
    "\n",
    "        res = pl.conditional(\n",
    "            ps,\n",
    "            event_condition = ps.df[\"Smoker\"] == True,\n",
    "            given_condition = ps.df[\"Income\"] > 40000\n",
    "        )\n",
    "               \n",
    "        chunk_results = pl.process_in_chunks(\n",
    "                \"example.csv\",\n",
    "                m,\n",
    "                pl.conditional,\n",
    "                lambda df: df[\"Smoker\"] == True,\n",
    "                lambda df: df[\"Income\"] > 40000\n",
    "            )\n",
    "        \n",
    "        combined_conditional = pl.combine_conditional(chunk_results)\n",
    "\n",
    "    if x==3:\n",
    "\n",
    "        res = pl.conditional(\n",
    "            ps,\n",
    "            event_condition = ps.df[\"CreditScore\"] < 650,\n",
    "            given_condition = ps.df[\"Income\"] > 40000\n",
    "        )\n",
    "               \n",
    "        chunk_results = pl.process_in_chunks(\n",
    "                \"example.csv\",\n",
    "                m,\n",
    "                pl.conditional,\n",
    "                lambda df: df[\"CreditScore\"] < 650,\n",
    "                lambda df: df[\"Income\"] > 40000\n",
    "            )\n",
    "        \n",
    "        combined_conditional = pl.combine_conditional(chunk_results)\n",
    "\n",
    "    if x==4:\n",
    "\n",
    "        res = pl.conditional(\n",
    "            ps,\n",
    "            event_condition = ps.df[\"Age\"] < 65,\n",
    "            given_condition = ps.df[\"Income\"] > 40000\n",
    "        )\n",
    "               \n",
    "        chunk_results = pl.process_in_chunks(\n",
    "                \"example.csv\",\n",
    "                m,\n",
    "                pl.conditional,\n",
    "                lambda df: df[\"Age\"] < 65,\n",
    "                lambda df: df[\"Income\"] > 40000\n",
    "            )\n",
    "        \n",
    "        combined_conditional = pl.combine_conditional(chunk_results)\n",
    "\n",
    "    if x==5:\n",
    "\n",
    "        res = pl.conditional(\n",
    "            ps,\n",
    "            event_condition = ps.df[\"GenderNumeric\"] == 0,\n",
    "            given_condition = ps.df[\"Gender\"] == \"Male\"\n",
    "        )\n",
    "               \n",
    "        chunk_results = pl.process_in_chunks(\n",
    "                \"example.csv\",\n",
    "                m,\n",
    "                pl.conditional,\n",
    "                lambda df: df[\"GenderNumeric\"] == 0,\n",
    "                lambda df: df[\"Gender\"] == \"Male\"\n",
    "            )\n",
    "        \n",
    "        combined_conditional = pl.combine_conditional(chunk_results)\n",
    "\n",
    "    \n",
    "    \n",
    "    error = abs(res - combined_conditional) / ((res + combined_conditional) / 2) * 100\n",
    "    errors_conditional.append(error)\n",
    "    \n",
    "    print(f\"Iteration {i} completed, chunk size={m}, error={error:.5f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b6d1464-fdec-4a8c-ba2f-b527ed3a257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Average % error: 0.000000000000022%\n",
      "📉 Min error: 0.000000000000000%, Max error: 0.000000000000063%\n"
     ]
    }
   ],
   "source": [
    "print(f\"📊 Average % error: {np.mean(errors_conditional):.15f}%\")\n",
    "print(f\"📉 Min error: {np.min(errors_conditional):.15f}%, Max error: {np.max(errors_conditional):.15f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b170b-4697-4a46-a071-8d73c9e85c75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#same process for joint probability\n",
    "\n",
    "errors_joint_probability = []\n",
    "chunk_sizes_joint_probability = []\n",
    "n = []\n",
    "\n",
    "for i in range(100):\n",
    "    x = random.choice([1, 2, 3, 4, 5])\n",
    "    n.append(x)\n",
    "    y= random.choice([\"dependent\", \"independent\"])\n",
    "    \n",
    "    m = np.random.randint(1000, 250000)\n",
    "    chunk_sizes_joint_probability.append((m/500000)*100)\n",
    "\n",
    "    if x==1:\n",
    "\n",
    "        res = pl.joint_probability(\n",
    "            ps,\n",
    "            ps.df[\"Smoker\"] == True,\n",
    "            ps.df[\"GenderNumeric\"] == 0,\n",
    "            con=y\n",
    "        )\n",
    "               \n",
    "        chunk_results = pl.process_in_chunks(\n",
    "                \"example.csv\",\n",
    "                m,\n",
    "                pl.joint_probability,\n",
    "                lambda df: df[\"Smoker\"] == True,\n",
    "                lambda df: df[\"GenderNumeric\"] == 0,\n",
    "                con=y\n",
    "            )\n",
    "        \n",
    "    if x==2:\n",
    "\n",
    "        res = pl.joint_probability(\n",
    "            ps,\n",
    "            ps.df[\"Smoker\"] == True,\n",
    "            ps.df[\"Income\"] > 40000,\n",
    "            con=y\n",
    "        )\n",
    "               \n",
    "        chunk_results = pl.process_in_chunks(\n",
    "                \"example.csv\",\n",
    "                m,\n",
    "                pl.joint_probability,\n",
    "                lambda df: df[\"Smoker\"] == True,\n",
    "                lambda df: df[\"Income\"] > 40000,\n",
    "                con=y\n",
    "            )\n",
    "        \n",
    "\n",
    "    if x==3:\n",
    "\n",
    "        res = pl.joint_probability(\n",
    "            ps,\n",
    "            ps.df[\"CreditScore\"] < 650,\n",
    "            ps.df[\"Income\"] > 40000,\n",
    "            con=y\n",
    "        )\n",
    "               \n",
    "        chunk_results = pl.process_in_chunks(\n",
    "                \"example.csv\",\n",
    "                m,\n",
    "                pl.joint_probability,\n",
    "                lambda df: df[\"CreditScore\"] < 650,\n",
    "                lambda df: df[\"Income\"] > 40000,\n",
    "                con=y\n",
    "            )\n",
    "        \n",
    "\n",
    "    if x==4:\n",
    "\n",
    "        res = pl.joint_probability(\n",
    "            ps,\n",
    "            ps.df[\"Age\"] < 65,\n",
    "            ps.df[\"Income\"] > 40000,\n",
    "            con=y\n",
    "        )\n",
    "               \n",
    "        chunk_results = pl.process_in_chunks(\n",
    "                \"example.csv\",\n",
    "                m,\n",
    "                pl.joint_probability,\n",
    "                lambda df: df[\"Age\"] < 65,\n",
    "                lambda df: df[\"Income\"] > 40000,\n",
    "                con=y\n",
    "            )\n",
    "        \n",
    "\n",
    "    if x==5:\n",
    "\n",
    "        res = pl.joint_probability(\n",
    "            ps,\n",
    "            ps.df[\"GenderNumeric\"] == 0,\n",
    "            ps.df[\"Gender\"] == \"Male\",\n",
    "            con=y\n",
    "        )\n",
    "               \n",
    "        chunk_results = pl.process_in_chunks(\n",
    "                \"example.csv\",\n",
    "                m,\n",
    "                pl.joint_probability,\n",
    "                lambda df: df[\"GenderNumeric\"] == 0,\n",
    "                lambda df: df[\"Gender\"] == \"Male\",\n",
    "                con=y\n",
    "            )\n",
    "        \n",
    "    combined_joint_probability = pl.combine_joint_probability(chunk_results, con=y)\n",
    "\n",
    "    \n",
    "    \n",
    "    error = abs(res - combined_joint_probability) / ((res + combined_joint_probability) / 2) * 100\n",
    "    errors_joint_probability.append(error)\n",
    "    \n",
    "    print(f\"Iteration {i} completed, chunk size={m}, error={error:.5f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eec1064a-055e-4abe-b16b-f604a752504f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Average % error: 0.000000000000013%\n",
      "📉 Min error: 0.000000000000000%, Max error: 0.000000000000081%\n"
     ]
    }
   ],
   "source": [
    "print(f\"📊 Average % error: {np.mean(errors_joint_probability):.15f}%\")\n",
    "print(f\"📉 Min error: {np.min(errors_joint_probability):.15f}%, Max error: {np.max(errors_joint_probability):.15f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7e7a6-05b3-4ddb-8728-3bf1d504e809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#same process for PDF_or_PMF (discrete variables)\n",
    "\n",
    "errors_PDF_or_PMF = []\n",
    "rel_errors_PDF_or_PMF = []\n",
    "chunk_sizes_PDF_or_PMF = []\n",
    "\n",
    "variables = [\"Age\", \"Gender\", \"GenderNumeric\", \"Smoker\"]\n",
    "\n",
    "for i in range(100):\n",
    "    discrete_var = random.choice(variables)  \n",
    "\n",
    "    full_pmf = pl.PDF_or_PMF(ps, discrete_var, var_type=\"discrete\", plot=False)\n",
    "\n",
    "    m = np.random.randint(1000, 250000)\n",
    "    chunk_sizes_PDF_or_PMF.append((m / 500000) * 100)\n",
    "    \n",
    "    results = pl.process_in_chunks(\n",
    "        \"example.csv\",  \n",
    "        m,              \n",
    "        pl.PDF_or_PMF,  \n",
    "        discrete_var,\n",
    "        var_type=\"discrete\",\n",
    "        plot=False\n",
    "    )\n",
    "\n",
    "    combined_pmf = pl.combine_PDF_or_PMF(results, var_type=\"discrete\", plot=False)\n",
    "\n",
    "    #calculate absolute and relative error\n",
    "    abs_error = np.mean(abs(full_pmf - combined_pmf))\n",
    "    rel_error = np.mean(abs(full_pmf - combined_pmf) / ((full_pmf + combined_pmf)/2) * 100)\n",
    "    \n",
    "    errors_PDF_or_PMF.append(abs_error)\n",
    "    rel_errors_PDF_or_PMF.append(rel_error)\n",
    "    \n",
    "    print(f\"Iteration {i} | Variable: {discrete_var} | Chunk size: {m} | Abs Error: {abs_error:.6f} | Rel Error: {rel_error:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "329174e0-e9b6-4e23-8f1b-e120b2cf7a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Average % error (discrete): 0.0624379191%\n",
      "📉 Min error: 0.0001791912%, Max error: 0.3309506726%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n📊 Average % error (discrete): {np.mean(rel_errors_PDF_or_PMF):.10f}%\")\n",
    "print(f\"📉 Min error: {np.min(rel_errors_PDF_or_PMF):.10f}%, Max error: {np.max(rel_errors_PDF_or_PMF):.10f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e725eb0-9cb0-4c3a-8aa9-93ec56921093",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(chunk_sizes_PDF_or_PMF, rel_errors_PDF_or_PMF, alpha=0.6)\n",
    "plt.xlabel(\"chunk_size\")\n",
    "plt.ylabel(\"Percentage error (%)\")\n",
    "plt.title(\"Effect of chunking proportion on percentage error for PDF_or_PMF (discrete variables)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"chunk_error_plot_PDF_or_PMF(discrete).png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4b3215-df5b-4d66-b0a7-0196f2182250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#same process for PDF_or_PMF (continuous variables)\n",
    "\n",
    "n_samples = 500000\n",
    "var_type = \"continuous\"\n",
    "errors = []\n",
    "iteration_times = []\n",
    "chunk_sizes = []\n",
    "var_list = []\n",
    "variables = [\"Age\", \"Income\", \"LoanAmount\", \"CreditScore\"]\n",
    "\n",
    "for i in range(100):\n",
    "    iter_start = time.time()\n",
    "\n",
    "    variable = random.choice(variables)\n",
    "\n",
    "    var_list.append(variable)\n",
    "\n",
    "    full_grid, full_pdf = pl.PDF_or_PMF(ps, variable, var_type=var_type, plot=False)\n",
    "\n",
    "    sd = pl.variance(ps, variable)**0.5\n",
    "    iqr = pl.IQR(ps, variable)[\"IQR\"] \n",
    "    global_min = ps.df[variable].min()\n",
    "    global_max = ps.df[variable].max() \n",
    "    \n",
    "    m = np.random.randint(1000, 250000)\n",
    "    chunk_sizes.append(m)\n",
    "    \n",
    "    results = pl.process_in_chunks(\n",
    "        \"example.csv\",\n",
    "        m,\n",
    "        pl.PDF_or_PMF,\n",
    "        variable,\n",
    "        var_type=var_type,\n",
    "        sd=sd,\n",
    "        iqr=iqr,\n",
    "        n_samples=n_samples,\n",
    "        global_min=global_min,\n",
    "        global_max=global_max,\n",
    "        grid_size=500,\n",
    "        plot=False\n",
    "    )\n",
    "    \n",
    "    combined_grid, combined_pdf = pl.combine_PDF_or_PMF(results, var_type=var_type, plot=False)\n",
    "    print(np.allclose(full_grid, combined_grid))\n",
    "    \n",
    "    # Compare PDFs directly\n",
    "    error = np.mean(abs(full_pdf - combined_pdf) / ((full_pdf + combined_pdf) / 2) * 100)\n",
    "    errors.append(error)\n",
    "    \n",
    "    iter_end = time.time()\n",
    "    iteration_times.append(iter_end - iter_start)\n",
    "    \n",
    "    print(f\"Iteration {i} | Chunk size: {m} | % error: {error:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a49e9efd-ff0d-4f1a-8b46-67ee1d322c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Average % error (continuous): 27.3951248511%\n",
      "📉 Min error: 5.6903812432%, Max error: 67.7206653584%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n📊 Average % error (continuous): {np.mean(errors):.10f}%\")\n",
    "print(f\"📉 Min error: {np.min(errors):.10f}%, Max error: {np.max(errors):.10f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb37f5-a71f-4162-a4de-227fe475c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot graph showing percentage error for each variable against chunk size\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for var in variables:\n",
    "    var_chunk_sizes = [chunk_sizes[i] for i in range(len(chunk_sizes)) if var_list[i] == var]\n",
    "    var_errors = [errors[i] for i in range(len(errors)) if var_list[i] == var]\n",
    "    \n",
    "    plt.scatter(var_chunk_sizes, var_errors, alpha=0.6, label=var)\n",
    "\n",
    "plt.xlabel(\"Chunk size\")\n",
    "plt.ylabel(\"Percentage error (%)\")\n",
    "plt.title(\"Chunk size vs. PDF % error (by variable)\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Variable\")\n",
    "plt.savefig(\"chunk_size_vs_error_by_variable.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0efae2d-d8b9-4a23-b2cc-65d8eba361d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs_by_m = {}\n",
    "variable = \"Income\"\n",
    "sd = pl.variance(ps, variable)**0.5\n",
    "iqr = pl.IQR(ps, variable)[\"IQR\"] \n",
    "global_min = ps.df[variable].min()\n",
    "global_max = ps.df[variable].max() \n",
    "\n",
    "chunk_sizes_to_test = [5000, 50000, 100000, 250000]\n",
    "pdfs_by_m = {}\n",
    "\n",
    "# full PDF for comparison\n",
    "full_grid, full_pdf = pl.PDF_or_PMF(ps, variable, var_type=\"continuous\", plot=False)\n",
    "\n",
    "# loop over different chunk sizes\n",
    "for m in chunk_sizes_to_test:\n",
    "    results = pl.process_in_chunks(\n",
    "        \"example.csv\",\n",
    "        m,\n",
    "        pl.PDF_or_PMF,\n",
    "        variable,\n",
    "        var_type=\"continuous\",\n",
    "        sd=sd,\n",
    "        iqr=iqr,\n",
    "        n_samples=500000,\n",
    "        global_min=global_min,\n",
    "        global_max=global_max,\n",
    "        grid_size=500\n",
    "    )\n",
    "    combined_grid, combined_pdf = pl.combine_PDF_or_PMF(results, var_type=\"continuous\", plot=False)\n",
    "    pdfs_by_m[m] = (combined_grid, combined_pdf)\n",
    "\n",
    "# plot in grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharey=True)\n",
    "axes = axes.flatten()  # flatten to 1D for easy looping\n",
    "\n",
    "for ax, (m, (grid, pdf)) in zip(axes, pdfs_by_m.items()):\n",
    "    ax.plot(grid, pdf, label=f\"Combined ({m})\")\n",
    "    ax.plot(full_grid, full_pdf, linestyle=\"--\", color=\"red\", label=\"Full PDF\")\n",
    "    \n",
    "    pct = m / 500000 * 100  # percentage relative to 500k\n",
    "    ax.set_title(f\"Chunk size = {pct:.0f}%\")\n",
    "    \n",
    "    ax.set_xlabel(\"Income\")\n",
    "    ax.grid(True)\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "    if ax == axes[0]:\n",
    "        ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "\n",
    "fig.suptitle(\"Effect of Chunk Size on PDF Accuracy\", fontsize=16, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"PDFs_computed_using_different_chunk_sizes.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef26199-cd14-40b6-b659-7cd69117c7b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#same process for EDF_or_CDF (discrete variables)\n",
    "\n",
    "errors_edf = []\n",
    "chunk_sizes_edf = []\n",
    "iteration_times_edf = []\n",
    "vars_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    iter_start = time.time()\n",
    "\n",
    "    discrete_var = random.choice([\"Age\", \"CreditScore\", \"GenderNumeric\", \"Income\"])\n",
    "    vars_list.append(discrete_var)\n",
    "\n",
    "    full_x, full_edf = pl.EDF_or_CDF(ps, discrete_var, var_type=\"discrete\", plot=False)\n",
    "\n",
    "    m = np.random.randint(1000, 250000)\n",
    "    chunk_sizes_edf.append(m)\n",
    "\n",
    "    results = pl.process_in_chunks(\n",
    "        \"example.csv\",\n",
    "        m,\n",
    "        pl.EDF_or_CDF,\n",
    "        discrete_var,\n",
    "        var_type=\"discrete\"\n",
    "    )\n",
    "\n",
    "\n",
    "    combined_x, combined_edf = pl.combine_EDF_or_CDF(results, var_type=\"discrete\", plot=False)\n",
    "    \n",
    "\n",
    "    all_x = full_x\n",
    "    full_interp = full_edf\n",
    "    combined_interp = combined_edf\n",
    "\n",
    "    error = np.mean(np.abs(full_interp - combined_interp) /\n",
    "                    ((full_interp + combined_interp) / 2) * 100)\n",
    "    errors_edf.append(error)\n",
    "    \n",
    "    iter_end = time.time()\n",
    "    iteration_times_edf.append(iter_end - iter_start)\n",
    "    \n",
    "    print(f\"Iteration {i} | Chunk size: {m} | % error: {error:.10f}\")\n",
    "\n",
    "\n",
    "average_edf_error = np.mean(errors_edf)\n",
    "print(f\"\\n📊 Average % error (EDF): {average_edf_error:.10f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87ec95c8-0b2f-413b-b179-ed0219dad9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Average % error (CDF): 0.0000000000%\n",
      "📉 Min error: 0.000000000000009%, Max error: 0.000000000003261%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n📊 Average % error (CDF): {average_edf_error:.10f}%\")\n",
    "print(f\"📉 Min error: {np.min(errors_edf):.15f}%, Max error: {np.max(errors_edf):.15f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cba99e-8f97-455f-9305-c36f8ad1a213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#same process for EDF_or_CDF (continuous variables)\n",
    "\n",
    "variables = [\"Age\", \"Income\", \"CreditScore\", \"LoanAmount\"]\n",
    "errors_cdf = []\n",
    "chunk_sizes_cdf = []\n",
    "var_list = []\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    var = random.choice(variables)\n",
    "    var_list.append(var)\n",
    "\n",
    "    global_min = ps.df[var].min()\n",
    "    global_max = ps.df[var].max()\n",
    "\n",
    "    full_x, full_cdf = pl.EDF_or_CDF(\n",
    "        ps, var,\n",
    "        var_type=\"continuous\",\n",
    "        plot=False\n",
    "    )\n",
    "\n",
    "    m = np.random.randint(1000, 250000)\n",
    "    chunk_sizes_cdf.append(m)\n",
    "\n",
    "    results = pl.process_in_chunks(\n",
    "        \"example.csv\",\n",
    "        m,\n",
    "        pl.EDF_or_CDF,\n",
    "        var,\n",
    "        var_type=\"continuous\",\n",
    "        global_min=global_min,\n",
    "        global_max=global_max\n",
    "    )\n",
    "\n",
    "    combined_x, combined_cdf = pl.combine_EDF_or_CDF(\n",
    "        results,\n",
    "        var_type=\"continuous\",\n",
    "        plot=False\n",
    "    )\n",
    "\n",
    "    error = np.mean(\n",
    "        np.abs(full_cdf - combined_cdf) /\n",
    "        ((full_cdf + combined_cdf) / 2) * 100\n",
    "    )\n",
    "    errors_cdf.append(error)\n",
    "\n",
    "    print(f\"Iteration {i} | Var: {var} | Chunk size: {m} | % error: {error:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "438aa71b-a289-4761-a5b1-40a2474487f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Average % error (CDF): 0.0000000000%\n",
      "📉 Min error: 0.000000000000365%, Max error: 0.000000000001406%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n📊 Average % error (CDF): {np.mean(errors_cdf):.10f}%\")\n",
    "print(f\"📉 Min error: {np.min(errors_cdf):.15f}%, Max error: {np.max(errors_cdf):.15f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc21d057-e762-4810-8fc9-8ebf2cc45c76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#same process for independence (discrete variables)\n",
    "\n",
    "errors_independence_discrete = []\n",
    "chunk_sizes_independence_discrete = []\n",
    "abs_errors_independence_discrete = []\n",
    "variables = [\"Gender\", \"GenderNumeric\", \"Smoker\"]\n",
    "\n",
    "\n",
    "agreements = 0  \n",
    "\n",
    "for i in range(100):\n",
    "    iter_start = time.time()\n",
    "    \n",
    "    x = random.choice(variables)\n",
    "    y = random.choice([v for v in variables if v != x])\n",
    "\n",
    "    res = pl.independence(ps, x, y)[\"p_value\"]\n",
    "    \n",
    "    m = np.random.randint(1000, 250000)\n",
    "    chunk_sizes_independence_discrete.append(m)\n",
    "    \n",
    "    chunk_results = pl.process_in_chunks(\n",
    "        \"example.csv\", m, \n",
    "        pl.independence, \n",
    "        x,\n",
    "        y   \n",
    "    )\n",
    "    combined_independence = pl.combine_independence(chunk_results)[\"p_value\"]\n",
    "\n",
    "    abs_error = abs(res-combined_independence)\n",
    "    abs_errors_independence_discrete.append(abs_error)\n",
    "    \n",
    "    full_significant = res < 0.05\n",
    "    chunk_significant = combined_independence < 0.05\n",
    "    if full_significant == chunk_significant:\n",
    "        agreements += 1\n",
    "    \n",
    "    print(f\"Iteration {i} | Chunk size={m} | Absolute error={abs_error} | Agreement={full_significant == chunk_significant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "641497da-2924-4a6a-a23c-ff2666381b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agreement in significance decisions: 100.00%\n",
      "2.887716107555227e-10\n"
     ]
    }
   ],
   "source": [
    "# Final agreement percentage\n",
    "agreement_percentage = agreements / 100 * 100\n",
    "print(f\"\\nAgreement in significance decisions: {agreement_percentage:.2f}%\")\n",
    "print(np.mean(abs_errors_independence_discrete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b360d48-4b5d-4eb0-b1d7-059a7253ff65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#same process for independence (conitnuous variables)\n",
    "\n",
    "errors_independence_continuous = []\n",
    "chunk_sizes_independence_continuous = []\n",
    "abs_errors_independence_continuous = []\n",
    "variables = [\"Age\", \"Income\", \"LoanAmount\", \"CreditScore\"]\n",
    "\n",
    "\n",
    "agreements = 0  \n",
    "\n",
    "for i in range(100):\n",
    "    iter_start = time.time()\n",
    "    \n",
    "    x = random.choice(variables)\n",
    "    y = random.choice([v for v in variables if v != x])\n",
    "\n",
    "    bin_boundaries=pl.bin_continuous_columns(ps, [x, y], return_edges=True)\n",
    "\n",
    "    res = pl.independence(ps, x, y, bin_boundaries=bin_boundaries)[\"p_value\"]\n",
    "    \n",
    "    m = np.random.randint(1000, 250000)\n",
    "    chunk_sizes_independence_continuous.append(m)\n",
    "    \n",
    "    chunk_results = pl.process_in_chunks(\n",
    "        \"example.csv\", m, \n",
    "        pl.independence, \n",
    "        x,\n",
    "        y,\n",
    "        bin_boundaries=bin_boundaries\n",
    "    )\n",
    "    combined_independence = pl.combine_independence(chunk_results)[\"p_value\"]\n",
    "\n",
    "    abs_error = abs(res-combined_independence)\n",
    "    abs_errors_independence_continuous.append(abs_error)\n",
    "    \n",
    "    \n",
    "    full_significant = res < 0.05\n",
    "    chunk_significant = combined_independence < 0.05\n",
    "    if full_significant == chunk_significant:\n",
    "        agreements += 1\n",
    "    \n",
    "    print(f\"Iteration {i} | Chunk size={m} | Absolute error={abs_error} | Agreement={full_significant == chunk_significant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "95fdbf91-19b2-4031-a195-9776229e4c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agreement in significance decisions: 100.00%\n",
      "4.3331949248744606e-17\n"
     ]
    }
   ],
   "source": [
    "# Final agreement percentage\n",
    "agreement_percentage = agreements / 100 * 100\n",
    "print(f\"\\nAgreement in significance decisions: {agreement_percentage:.2f}%\")\n",
    "print(np.mean(abs_errors_independence_continuous))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf272f3-df61-4ac0-9b49-6872aff30ec5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#same process for mixed pairs of variables\n",
    "\n",
    "errors_independence_mixed = []\n",
    "chunk_sizes_independence_mixed = []\n",
    "abs_errors_independence_mixed = []\n",
    "continuous_variables = [\"Age\", \"Income\", \"LoanAmount\", \"CreditScore\"]\n",
    "discrete_variables = [\"Gender\", \"GenderNumeric\", \"Smoker\"]\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "agreements = 0  \n",
    "\n",
    "for i in range(100):\n",
    "    iter_start = time.time()\n",
    "    \n",
    "    x = random.choice(continuous_variables)\n",
    "    y = random.choice(discrete_variables)\n",
    "\n",
    "    bin_boundaries=pl.bin_continuous_columns(ps, [x], return_edges=True)\n",
    "\n",
    "    res = pl.independence(ps, x, y, bin_boundaries=bin_boundaries)[\"p_value\"]\n",
    "    \n",
    "    m = np.random.randint(1000, 250000)\n",
    "    chunk_sizes_independence_continuous.append(m)\n",
    "    \n",
    "    chunk_results = pl.process_in_chunks(\n",
    "        \"example.csv\", m, \n",
    "        pl.independence, \n",
    "        x,\n",
    "        y,\n",
    "        bin_boundaries=bin_boundaries\n",
    "    )\n",
    "    combined_independence = pl.combine_independence(chunk_results)[\"p_value\"]\n",
    "\n",
    "    abs_error = abs(res-combined_independence)\n",
    "    abs_errors_independence_mixed.append(abs_error)\n",
    "    \n",
    "    \n",
    "    # Compare significance decisions\n",
    "    full_significant = res < 0.05\n",
    "    chunk_significant = combined_independence < 0.05\n",
    "    if full_significant == chunk_significant:\n",
    "        agreements += 1\n",
    "    \n",
    "    print(f\"Iteration {i} | Chunk size={m} | Absolute error={abs_error} | Agreement={full_significant == chunk_significant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e0bde4a-95eb-41c9-89a2-dd2c77cd0a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agreement in significance decisions: 100.00%\n",
      "2.0381403302479902e-10\n"
     ]
    }
   ],
   "source": [
    "# Final agreement percentage\n",
    "agreement_percentage = agreements / 100 * 100\n",
    "print(f\"\\nAgreement in significance decisions: {agreement_percentage:.2f}%\")\n",
    "print(np.mean(abs_errors_independence_mixed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc3690-2bc0-4491-8ecb-faef072e56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find bin boundaries of continuous variables using chunked bin function\n",
    "bin_boundaries=pl.binning_from_chunks(\"example.csv\", 5000, continuous_columns=[\"Age\", \"CreditScore\", \"LoanAmount\", \"Income\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef1e97-5095-47de-b76a-9c0beca8e93d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run regular function 100 times to find most stable edges\n",
    "n_runs = 100\n",
    "edge_counter = Counter()\n",
    "\n",
    "for i in range(n_runs):\n",
    "    model, _ = pl.build_bayesian_network(\n",
    "        ps,\n",
    "        scoring_method='BIC',\n",
    "        n_samples=None,\n",
    "        scale_factor=1,\n",
    "        random_state=i,\n",
    "        bin_boundaries=bin_boundaries\n",
    "    )\n",
    "    edges = [tuple(sorted(edge)) for edge in model.edges()]\n",
    "    edge_counter.update(edges)\n",
    "    print(i)\n",
    "\n",
    "#find how many iterations the edge appeared in\n",
    "for edge, count in edge_counter.items():\n",
    "    print(f\"Edge {edge} appeared in {count}/{n_runs} runs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1b253f9-f484-4d02-9d20-ba446214e2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Age', 'CreditScore'): 1,\n",
       " ('Age', 'Income'): 1,\n",
       " ('Income', 'LoanAmount'): 1,\n",
       " ('CreditScore', 'Income'): 1,\n",
       " ('Income', 'smoker'): 1,\n",
       " ('CreditScore', 'LoanAmount'): 1,\n",
       " ('Age', 'smoker'): 1,\n",
       " ('gender', 'gender_numeric'): 1,\n",
       " ('gender_numeric', 'smoker'): 1}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return list of edges which appear in all 100 iterations\n",
    "stable_edges = {edge: 1 for edge, count in edge_counter.items() if count == n_runs}\n",
    "stable_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e07baeb-f5f3-4e8f-a4ca-7955624db28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('CreditScore', 'LoanAmount'), ('Income', 'CreditScore'), ('Age', 'smoker'), ('Age', 'CreditScore'), ('Income', 'LoanAmount'), ('gender_numeric', 'smoker'), ('gender', 'gender_numeric'), ('Age', 'Income')}\n"
     ]
    }
   ],
   "source": [
    "#list of correct edges\n",
    "correct_edges = {\n",
    "    ('Age', 'CreditScore'),\n",
    "    ('Income', 'CreditScore'),      \n",
    "    ('Age', 'Income'),\n",
    "    ('Income', 'LoanAmount'),\n",
    "    ('CreditScore', 'LoanAmount'),\n",
    "    ('Age', 'smoker'),\n",
    "    ('gender', 'gender_numeric'),  \n",
    "    ('gender_numeric', 'smoker'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "88349c1c-2fd3-4675-8b0e-4d6fc68c48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function which compares the edges found using the chunked processor to those in the correct DAG\n",
    "\n",
    "def score_combined_graph(learned_edges, stable_edges):\n",
    "    learned_set = set(tuple(edge) for edge in learned_edges if len(edge) == 2)\n",
    "    if isinstance(stable_edges, dict):\n",
    "        stable_set = set(tuple(edge) for edge in stable_edges.keys() if len(edge) == 2)\n",
    "    else:\n",
    "        stable_set = set(tuple(edge) for edge in stable_edges if len(edge) == 2)\n",
    "\n",
    "    matches = learned_set & stable_set\n",
    "    reversed_edges = set((v, u) for (u, v) in learned_set if (v, u) in stable_set)\n",
    "    missing_edges = stable_set - learned_set - reversed_edges\n",
    "    extra_edges = learned_set - stable_set - reversed_edges\n",
    "\n",
    "    accuracy = len(matches) / len(stable_set) if stable_set else 0\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"matches\": matches,\n",
    "        \"reversed_edges\": reversed_edges,\n",
    "        \"missing_edges\": missing_edges,\n",
    "        \"extra_edges\": extra_edges,\n",
    "        \"total_edges\": len(stable_set)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1bcaa2-6def-497d-b5af-053070184fef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#measure accuracy of combine_bayesian_models when using the hybrid method\n",
    "accuracy = []\n",
    "chunk_size = []\n",
    "sample_size = []\n",
    "incorrect_edges = []\n",
    "reversed_edges = []\n",
    "related_edges = []\n",
    "total_edges_list = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(100):\n",
    "    m = np.random.randint(1000, 250000)\n",
    "    y = np.random.random()\n",
    "\n",
    "    results = pl.process_in_chunks(\n",
    "        \"example.csv\",\n",
    "        func=pl.build_bayesian_network_edges,\n",
    "        chunk_size=m,\n",
    "        sample_size=int(m*y),\n",
    "        bin_boundaries=bin_boundaries,\n",
    "        scoring_method=\"BIC\"\n",
    "    )\n",
    "\n",
    "    sorted_edges = pl.combine_bayesian_models(results, top_k=8)\n",
    "\n",
    "    dict_scores = score_combined_graph(sorted_edges, correct_edges)\n",
    "\n",
    "    accuracy.append(dict_scores[\"accuracy\"])\n",
    "    chunk_size.append(m)\n",
    "    sample_size.append(y)\n",
    "    incorrect_edges.append(len(dict_scores[\"missing_edges\"]) + len(dict_scores[\"extra_edges\"]))\n",
    "    reversed_edges.append(len(dict_scores[\"reversed_edges\"]))\n",
    "    related_edges.append(len(dict_scores[\"reversed_edges\"]) + len(dict_scores[\"matches\"]))\n",
    "    total_edges_list.append(dict_scores[\"total_edges\"])\n",
    "\n",
    "    print(f\"Iteration {i} completed\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"⏱️ Total processing time: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f3dd0bce-9051-4f8d-af10-1897503ff64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of correctly learned edges (excluding reversed): 3.82\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>sample_fraction</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>incorrect_edges</th>\n",
       "      <th>reversed_edges</th>\n",
       "      <th>related_edges</th>\n",
       "      <th>total_edges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>126012.190000</td>\n",
       "      <td>0.524294</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>6.590000</td>\n",
       "      <td>3.990000</td>\n",
       "      <td>7.810000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>70892.643964</td>\n",
       "      <td>0.293125</td>\n",
       "      <td>0.080206</td>\n",
       "      <td>1.223466</td>\n",
       "      <td>0.688946</td>\n",
       "      <td>0.442559</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3568.000000</td>\n",
       "      <td>0.006952</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>69976.500000</td>\n",
       "      <td>0.275032</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>125236.000000</td>\n",
       "      <td>0.536716</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>189421.750000</td>\n",
       "      <td>0.779733</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>248255.000000</td>\n",
       "      <td>0.997740</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          chunk_size  sample_fraction    accuracy  incorrect_edges  \\\n",
       "count     100.000000       100.000000  100.000000       100.000000   \n",
       "mean   126012.190000         0.524294    0.477500         6.590000   \n",
       "std     70892.643964         0.293125    0.080206         1.223466   \n",
       "min      3568.000000         0.006952    0.375000         4.000000   \n",
       "25%     69976.500000         0.275032    0.375000         6.000000   \n",
       "50%    125236.000000         0.536716    0.500000         7.000000   \n",
       "75%    189421.750000         0.779733    0.500000         7.000000   \n",
       "max    248255.000000         0.997740    0.750000         9.000000   \n",
       "\n",
       "       reversed_edges  related_edges  total_edges  \n",
       "count      100.000000     100.000000        100.0  \n",
       "mean         3.990000       7.810000          8.0  \n",
       "std          0.688946       0.442559          0.0  \n",
       "min          2.000000       6.000000          8.0  \n",
       "25%          4.000000       8.000000          8.0  \n",
       "50%          4.000000       8.000000          8.0  \n",
       "75%          4.000000       8.000000          8.0  \n",
       "max          5.000000       8.000000          8.0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"chunk_size\": chunk_size,\n",
    "    \"sample_fraction\": sample_size,\n",
    "    \"accuracy\": accuracy,\n",
    "    \"incorrect_edges\": incorrect_edges,\n",
    "    \"reversed_edges\": reversed_edges,\n",
    "    \"related_edges\": related_edges,\n",
    "    \"total_edges\": total_edges_list\n",
    "})\n",
    "\n",
    "correct_edges_abs = np.array(accuracy) * np.array(total_edges_list)\n",
    "\n",
    "avg_correct_edges = correct_edges_abs.mean()\n",
    "\n",
    "print(f\"Average number of correctly learned edges (excluding reversed): {avg_correct_edges:.2f}\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf218f-6e2b-4a1a-9dd8-2b191d732c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(chunk_size, accuracy)\n",
    "plt.xlabel(\"Chunk size\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Chunk size vs Accuracy\")\n",
    "plt.savefig(\"chunk_size_vs_accuracy_hybrid.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sample_size, accuracy)\n",
    "plt.xlabel(\"Sample size fraction\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Sample size vs Accuracy\")\n",
    "plt.savefig(\"sample_size_vs_accuracy_hybrid.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b9c56-d0cc-4431-a870-e94f303f2259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#same process when using frequency method\n",
    "accuracy_frequency = []\n",
    "chunk_size_frequency = []\n",
    "sample_size_frequency = []\n",
    "incorrect_edges_frequency = []\n",
    "reversed_edges_frequency = []\n",
    "related_edges_frequency = []\n",
    "total_edges_list_frequency = []\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    m = np.random.randint(1000, 250000)\n",
    "    y = np.random.random()\n",
    "\n",
    "    results = pl.process_in_chunks(\n",
    "        \"example.csv\",\n",
    "        func=pl.build_bayesian_network_edges,\n",
    "        chunk_size=m,\n",
    "        sample_size=int(m*y),\n",
    "        bin_boundaries=bin_boundaries,\n",
    "        scoring_method=\"BIC\"\n",
    "    )\n",
    "\n",
    "    sorted_edges = pl.combine_bayesian_models(results, top_k=8, score_type=\"frequency\")\n",
    "\n",
    "    dict_scores = score_combined_graph(sorted_edges, correct_edges)\n",
    "\n",
    "    accuracy_frequency.append(dict_scores[\"accuracy\"])\n",
    "    chunk_size_frequency.append(m)\n",
    "    sample_size_frequency.append(y)\n",
    "    incorrect_edges_frequency.append(len(dict_scores[\"missing_edges\"]) + len(dict_scores[\"extra_edges\"]))\n",
    "    reversed_edges_frequency.append(len(dict_scores[\"reversed_edges\"]))\n",
    "    related_edges_frequency.append(len(dict_scores[\"reversed_edges\"]) + len(dict_scores[\"matches\"]))\n",
    "    total_edges_list_frequency.append(dict_scores[\"total_edges\"])\n",
    "\n",
    "    print(f\"Iteration {i} completed\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"⏱️ Total processing time: {elapsed:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3c2d87a5-d567-4492-b47d-f393e964dd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of correctly learned edges (excluding reversed): 3.63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>sample_fraction</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>incorrect_edges</th>\n",
       "      <th>reversed_edges</th>\n",
       "      <th>related_edges</th>\n",
       "      <th>total_edges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>131280.150000</td>\n",
       "      <td>0.516096</td>\n",
       "      <td>0.453750</td>\n",
       "      <td>5.160000</td>\n",
       "      <td>3.450000</td>\n",
       "      <td>7.080000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>66449.165036</td>\n",
       "      <td>0.270651</td>\n",
       "      <td>0.098369</td>\n",
       "      <td>1.220366</td>\n",
       "      <td>0.783349</td>\n",
       "      <td>0.691653</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4420.000000</td>\n",
       "      <td>0.014393</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>85397.750000</td>\n",
       "      <td>0.334235</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>131828.000000</td>\n",
       "      <td>0.520286</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>189151.500000</td>\n",
       "      <td>0.703703</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>249710.000000</td>\n",
       "      <td>0.994457</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          chunk_size  sample_fraction    accuracy  incorrect_edges  \\\n",
       "count     100.000000       100.000000  100.000000       100.000000   \n",
       "mean   131280.150000         0.516096    0.453750         5.160000   \n",
       "std     66449.165036         0.270651    0.098369         1.220366   \n",
       "min      4420.000000         0.014393    0.250000         2.000000   \n",
       "25%     85397.750000         0.334235    0.375000         4.000000   \n",
       "50%    131828.000000         0.520286    0.500000         5.000000   \n",
       "75%    189151.500000         0.703703    0.500000         6.000000   \n",
       "max    249710.000000         0.994457    0.750000         9.000000   \n",
       "\n",
       "       reversed_edges  related_edges  total_edges  \n",
       "count      100.000000     100.000000        100.0  \n",
       "mean         3.450000       7.080000          8.0  \n",
       "std          0.783349       0.691653          0.0  \n",
       "min          2.000000       5.000000          8.0  \n",
       "25%          3.000000       7.000000          8.0  \n",
       "50%          3.000000       7.000000          8.0  \n",
       "75%          4.000000       7.250000          8.0  \n",
       "max          5.000000       8.000000          8.0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_frequency = pd.DataFrame({\n",
    "    \"chunk_size\": chunk_size_frequency,\n",
    "    \"sample_fraction\": sample_size_frequency,\n",
    "    \"accuracy\": accuracy_frequency,\n",
    "    \"incorrect_edges\": incorrect_edges_frequency,\n",
    "    \"reversed_edges\": reversed_edges_frequency,\n",
    "    \"related_edges\": related_edges_frequency,\n",
    "    \"total_edges\": total_edges_list_frequency\n",
    "})\n",
    "\n",
    "correct_edges_abs_frequency = np.array(accuracy_frequency) * np.array(total_edges_list_frequency)\n",
    "\n",
    "avg_correct_edges_frequency = correct_edges_abs_frequency.mean()\n",
    "\n",
    "print(f\"Average number of correctly learned edges (excluding reversed): {avg_correct_edges_frequency:.2f}\")\n",
    "df_frequency.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65ce71-7ef4-4975-95b6-1586325c9e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(chunk_size_frequency, accuracy_frequency)\n",
    "plt.xlabel(\"Chunk size\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Chunk size vs Accuracy\")\n",
    "plt.savefig(\"chunk_size_vs_accuracy_frequency.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sample_size_frequency, accuracy_frequency)\n",
    "plt.xlabel(\"Sample size fraction\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Sample size vs Accuracy\")\n",
    "plt.savefig(\"sample_size_vs_accuracy_hybrid.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a9c8a-b922-40cf-b5b2-afb8adb1007d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#same for softmax method\n",
    "accuracy_softmax = []\n",
    "chunk_size_softmax = []\n",
    "sample_size_softmax = []\n",
    "incorrect_edges_softmax = []\n",
    "reversed_edges_softmax = []\n",
    "related_edges_softmax = []\n",
    "total_edges_list_softmax = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(100):\n",
    "    m = np.random.randint(1000, 250000)\n",
    "    y = np.random.random()\n",
    "\n",
    "    results = pl.process_in_chunks(\n",
    "        \"example.csv\",\n",
    "        func=pl.build_bayesian_network_edges,\n",
    "        chunk_size=m,\n",
    "        sample_size=int(m*y),\n",
    "        bin_boundaries=bin_boundaries,\n",
    "        handle_missing=\"drop\",\n",
    "        scoring_method=\"BIC\"\n",
    "    )\n",
    "\n",
    "    sorted_edges = pl.combine_bayesian_models(results, top_k=8, score_type=\"softmax\")\n",
    "\n",
    "    dict_scores = score_combined_graph(sorted_edges, correct_edges)\n",
    "\n",
    "    accuracy_softmax.append(dict_scores[\"accuracy\"])\n",
    "    chunk_size_softmax.append(m)\n",
    "    sample_size_softmax.append(y)\n",
    "    incorrect_edges_softmax.append(len(dict_scores[\"missing_edges\"]) + len(dict_scores[\"extra_edges\"]))\n",
    "    reversed_edges_softmax.append(len(dict_scores[\"reversed_edges\"]))\n",
    "    related_edges_softmax.append(len(dict_scores[\"reversed_edges\"]) + len(dict_scores[\"matches\"]))\n",
    "    total_edges_list_softmax.append(dict_scores[\"total_edges\"])\n",
    "\n",
    "    print(f\"Iteration {i} completed\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"⏱️ Total processing time: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d29edda9-8d85-4006-bd1c-f6ece9a9f431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of correctly learned edges (excluding reversed): 2.77\n",
      "          chunk_size  sample_fraction   accuracy  incorrect_edges  \\\n",
      "count     100.000000       100.000000  100.00000       100.000000   \n",
      "mean   116187.760000         0.467512    0.34625         7.640000   \n",
      "std     74778.218621         0.295838    0.14308         2.560461   \n",
      "min      2062.000000         0.010838    0.12500         3.000000   \n",
      "25%     53534.500000         0.233158    0.25000         6.000000   \n",
      "50%    108415.500000         0.420857    0.37500         8.000000   \n",
      "75%    179809.750000         0.732995    0.50000         9.250000   \n",
      "max    242080.000000         0.995831    0.62500        13.000000   \n",
      "\n",
      "       reversed_edges  related_edges  total_edges  \n",
      "count       100.00000     100.000000        100.0  \n",
      "mean          2.14000       4.910000          8.0  \n",
      "std           0.85304       1.443166          0.0  \n",
      "min           0.00000       2.000000          8.0  \n",
      "25%           2.00000       4.000000          8.0  \n",
      "50%           2.00000       5.000000          8.0  \n",
      "75%           3.00000       6.000000          8.0  \n",
      "max           5.00000       8.000000          8.0  \n",
      "Maximum correct edges (not reversed) = 5\n"
     ]
    }
   ],
   "source": [
    "df_softmax = pd.DataFrame({\n",
    "    \"chunk_size\": chunk_size_softmax,\n",
    "    \"sample_fraction\": sample_size_softmax,\n",
    "    \"accuracy\": accuracy_softmax,\n",
    "    \"incorrect_edges\": incorrect_edges_softmax,\n",
    "    \"reversed_edges\": reversed_edges_softmax,\n",
    "    \"related_edges\": related_edges_softmax,\n",
    "    \"total_edges\": total_edges_list_softmax\n",
    "})\n",
    "\n",
    "correct_edges_abs_softmax = np.array(accuracy_softmax) * np.array(total_edges_list_softmax)\n",
    "\n",
    "avg_correct_edges_softmax= correct_edges_abs_softmax.mean()\n",
    "\n",
    "print(f\"Average number of correctly learned edges (excluding reversed): {avg_correct_edges_softmax:.2f}\")\n",
    "print(df_softmax.describe())\n",
    "\n",
    "matches_list = [related - reversed_ for related, reversed_ in zip(related_edges_softmax, reversed_edges_softmax)]\n",
    "\n",
    "max_correct_edges = max(matches_list)\n",
    "print(f\"Maximum correct edges (not reversed) = {max_correct_edges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9fc4c1-b0ea-453a-a0a8-9972262b58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(chunk_size_softmax, accuracy_softmax)\n",
    "plt.xlabel(\"Chunk size\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Chunk size vs Accuracy\")\n",
    "plt.savefig(\"chunk_size_vs_accuracy_softmax.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sample_size_softmax, accuracy_softmax)\n",
    "plt.xlabel(\"Sample size fraction\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Sample size vs Accuracy\")\n",
    "plt.savefig(\"sample_size_vs_accuracy_softmax.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2c1ba9a3-623c-41ac-ae7f-b1fcac49cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new function to compare correct edges to those returned by aggregate_and_build_network\n",
    "\n",
    "def score_edges(predicted_edges, correct_edges):\n",
    "\n",
    "    predicted_set = set(predicted_edges)\n",
    "    correct_set = set(correct_edges)\n",
    "    \n",
    "    matches = len(predicted_set & correct_set)\n",
    "    \n",
    "    reversed_edges = len([e for e in predicted_set if (e[1], e[0]) in correct_set])\n",
    "    \n",
    "    missing_edges = len(correct_set - predicted_set - {(e[1], e[0]) for e in predicted_set})\n",
    "    \n",
    "    extra_edges = len(predicted_set - correct_set - {(e[1], e[0]) for e in correct_set})\n",
    "    \n",
    "    accuracy = 100 * matches / len(correct_set) if correct_set else 0.0\n",
    "\n",
    "    accuracy_with_reversals = 100 * (matches + reversed_edges) / len(correct_set) if correct_set else 0.0\n",
    "    \n",
    "    return {\n",
    "        'matches': matches,\n",
    "        'reversed_edges': reversed_edges,\n",
    "        'missing_edges': missing_edges,\n",
    "        'extra_edges': extra_edges,\n",
    "        'accuracy': accuracy,\n",
    "        'accuracy_with_reversals': accuracy_with_reversals\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97adf4a-d27f-451d-b597-3c9668821457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#measure accuracy of aggregate_and_build_network\n",
    "accuracy_3 = []\n",
    "chunk_size_3 = []\n",
    "sample_size_3 = []\n",
    "incorrect_edges_3 = []\n",
    "missing_edges_3 = []\n",
    "extra_edges_3 = []\n",
    "reversed_edges_3 = []\n",
    "accuracy_with_reversals_3 = []\n",
    "related_edges_3 = []\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(50):\n",
    "    m = np.random.randint(1000, 250000)\n",
    "    y = np.random.random()\n",
    "\n",
    "    results = pl.process_in_chunks(\n",
    "        \"example.csv\",\n",
    "        func=pl.build_bayesian_network,\n",
    "        chunk_size=m,\n",
    "        sample_size=int(m*y),\n",
    "        handle_missing=\"drop\",\n",
    "        scoring_method=\"BIC\"\n",
    "    )\n",
    "\n",
    "      \n",
    "    consensus_edges, edge_scores = pl.aggregate_and_build_network(results, bin_boundaries=bin_boundaries)\n",
    "\n",
    "    dict = score_edges(consensus_edges, correct_edges)\n",
    "\n",
    "    accuracy_3.append(dict[\"accuracy\"])\n",
    "    chunk_size_3.append(m)\n",
    "    sample_size_3.append(y)\n",
    "    missing_edges_3.append(dict[\"missing_edges\"])\n",
    "    extra_edges_3.append(dict[\"missing_edges\"])\n",
    "    incorrect_edges_3.append(dict[\"missing_edges\"]+dict[\"extra_edges\"])\n",
    "    reversed_edges_3.append(dict[\"reversed_edges\"])\n",
    "    related_edges_3.append(dict[\"reversed_edges\"]+dict[\"matches\"])\n",
    "    accuracy_with_reversals_3.append(dict[\"accuracy_with_reversals\"])\n",
    "    \n",
    "    \n",
    "    print(f\"Iteration {i} completed\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b267b6-322a-4219-bd4e-a1d07bfe9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis of results\n",
    "print(np.mean(accuracy_3))\n",
    "print(np.mean(incorrect_edges_3))\n",
    "print(incorrect_edges_3)\n",
    "print(missing_edges_3)\n",
    "print(np.mean(accuracy_with_reversals_3))\n",
    "print(np.max(accuracy_with_reversals_3))\n",
    "print(np.mean(missing_edges_3))\n",
    "print(related_edges)\n",
    "print(np.mean(related_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d4948ac4-3db5-4be2-b634-6cac4ddaf8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4650.428697347641"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#time elapsed\n",
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3afaaf8-0d14-4680-9fe1-d414bcf19791",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(chunk_size_3, accuracy_3)\n",
    "plt.xlabel(\"Chunk size\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Chunk size vs Accuracy\")\n",
    "plt.savefig(\"chunk_size_vs_accuracy_bayesian_netowrk.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sample_size_3, accuracy_with_reversals_3)\n",
    "plt.xlabel(\"Sample size fraction\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Sample size vs Accuracy (related edges included)\")\n",
    "plt.savefig(\"related_edges_vs_chunk_size_bayesian_netowrk.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sample_size_3, accuracy_3)\n",
    "plt.xlabel(\"Sample size fraction\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Sample size vs Accuracy\")\n",
    "plt.savefig(\"Sample_size_vs_Accuracy_bayesian_netowrk.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
